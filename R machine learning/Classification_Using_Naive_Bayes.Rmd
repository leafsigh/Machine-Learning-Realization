---
title: "Probablistic Learning-Classification using Naive Bayes"
output: html_document
---

For instance, a common application of naive Bayes uses the frequency of words in past junk email messages to identify new junk mail. 

Classifiers based on Bayesian methods utilize training data to calculate an observed probability of each class based on feature values. When the classi er is used later on unlabeled data, it uses the observed probabilities to predict the most likely class for the new features.

Typically, Bayesian classifiers are best applied to problems in which the information from numerous attributes should be considered simultaneously in order to estimate the probability of an outcome. While many algorithms ignore features that have weak effects, Bayesian methods utilize all available evidence to subtly change the predictions. If a large number of features have relatively minor effects, taken together their combined impact could be quite large.

Naive Bayes Algo
1)pros and cons
strengths:
• Simple, fast, and very effective
• Does well with noisy and missing data
• Requires relatively few examples for training, but also works well with very large numbers of examples
• Easy to obtain the estimated probability for a prediction
weaknesses:
• Relies on an often-faulty assumption of equally important and independent features
• Not ideal for datasets with large numbers of numeric features
• Estimated probabilities are less reliable than the predicted classes

2)What naive means:
 it makes a couple of "naive" assumptions about the data. In particular, naive Bayes assumes that all of the features in the dataset are @equally-important and @independent. Also assumes @class-conditional-independence, which means that events are independent so long as they are conditioned on the same class value. These assumptions are rarely true in most of the real-world applications.

****
However, in most cases when these assumptions are violated, naive Bayes still performs fairly well. This is true even in extreme circumstances where strong dependencies are found among the features. 

3)Be familiar with Beyasian (posterior) probability:
[1] One feature: Viagra
P(spam|Viagra)=P(Viagra|spam)*P(spam)/P(Viagra)

The P(spam), P(Viagra) and P(Viagra|spam) can be estimated from plenty of emails. If there are 20 spams in 100 emails, P(spam)=20%. If 4 spams of 20 have Viagra, then P(Viagra|spam)=4/20. If 5 email of 100 have Viagra, then P(Viagra)=5/20.

[2] Multi features: Viagra, Money, Groceries and Unsubscribe
P(spam|W1&!W2&!W3&W4)=P(W1&!W2&!W3&W4|spam)*P(spam)/P(W1&!W2&!W3&W4)

Because Naive Bayes assumes class conditional independence:
P(spam|W1&!W2&!W3&W4)=
P(W1|spam)P(!W2|spam)P(!W3|spam)P(W4|spam)P(spam)/P(W1)P(!W2)P(!W3)P(W4)

Each probability on the right side can also be estimated by using the method mentioned before.

[3] Summarized Function
L---level(spam and ham)   F---feature(Viagra, Money, Grocery...)
P(C_L|F_1,F_2,F_3,...F_n) = 1/Z*p(C_L)sum(p(F_i|C_L), i=1,2,...n)

The numerator is overall likelihood of level L.

1/Z is scaling factor

4) The Laplace Estimator

5) Using numeric features with Naive Bayes



Example--filtering mobile phone spam with the naive Bayes Algorithm

1) Collecting data
```{r}
#reserve the data into 'sms'
download.file('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/sms_spam.csv',destfile = '/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/SMS Spam Collection/sms_spam.csv')
```
```{r}
sms<-read.csv('/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/SMS Spam Collection/sms_spam.csv',stringsAsFactors = FALSE)
sms
```
```{r}
str(sms)
```
```{r}
sms$type<-factor(sms$type)
```


Data preparation--processing text data for analysis
```{r}
# To handle the text, we need to consider how to remove numbers, punctuation, handle uninteresting words such as and, but, and or, and #how to break apart sentences into individual words. 

require(tm)
```
```{r}
# First, we need to build a corpus to collect the text documents
sms_corpus<-Corpus(VectorSource(sms$text))
print(sms_corpus)
# he Corpus() function creates an R object to store text documents. This function takes a parameter specifying the format of the text documents to be loaded. The Corpus() function is extremely  exible and can read documents from many different sources such as PDFs and Microsoft Word documents. 
```

```{r}
# To look at the 1~10 rows of the corpus, we can use the inspect() function.
inspect(sms_corpus[1:10])
```

```{r}
# we will need to perform some common cleaning steps in order to remove punctuation and other characters that may clutter the result.
# First, we will convert all of the SMS messages to lowercase and remove any numbers:
corpus_clean<-tm_map(sms_corpus, tolower)
corpus_clean<-tm_map(corpus_clean, removeNumbers)

# A common practice when analyzing text data is to remove  ller words such as to, and, but, and or. These are known as stop words. 
corpus_clean<-tm_map(corpus_clean, removeWords, stopwords())

# Move punctuation
corpus_clean<-tm_map(corpus_clean, removePunctuation)

# Now that we have removed numbers, stop words, and punctuation, the text messages are left with blank spaces where these characters used to be. The last step then is to remove additional whitespace, leaving only a single space between words.
corpus_clean<-tm_map(corpus_clean, stripWhitespace)


```

```{r}
# the next step is to split the messages into individual components through a process called tokenization. A token is a single element of a text string; in this case, the tokens are words.
sms_token<-as.matrix(DocumentTermMatrix(corpus_clean))

# *** as.matrix is very essential, because we need to wordcloud() the sparse matrix, and we need to input frequency parameter in wordcloud() function. And it's very convenient to use sort() to get frequency in a matrix.
```

```{r}
 # The DocumentTermMatrix() function will take a corpus and create a data structure called a sparse matrix, in which the rows of the matrix indicate documents (that is, SMS messages) and the columns indicate terms (that is, words). Each cell in the matrix stores a number indicating a count of the times the word indicated by the column appears in the document indicated by the row.
sms_token[1:10,1:10]
```


Data Preparation -- creating training and test datasets
```{r}
set.seed(330)
train<-sample(5574,5000)
train_raw_sms<-sms[train,]
test_raw_sms<-sms[-train,]
train_clean_sms<-sms_token[train,]
test_clean_sms<-sms_token[-train,]
```

```{r}
prop.table(table(train_raw_sms$type))
```
```{r}
prop.table(table(test_raw_sms$type))
```
```{r}
# Both the training data and test data contain about 13 percent spam. This suggests that the spam messages were divided evenly between the two datasets.
```


Visualizing text data -- word clouds
```{r}
# A word cloud is a way to visually depict the frequency at which words appear in text data. The cloud is made up of words scattered somewhat randomly around the  gure. Words appearing more often in the text are shown in a larger font, while less common terms are shown in smaller fonts.
require(wordcloud)
```


```{r}
# The following steps is the algo of applying wordcloud() function
v<-sort(colSums(sms_token), decreasing = TRUE)

# Let's see the first 14 lines of word frequency.
head(v,14)
```
```{r}
#reserve the word's name( from the text content ) in a vector
words<-names(v)
words[1:14]
```
```{r}
# Use the word's name and word's frequency to build a dataframe.
d<-data.frame(word = words, freq = v)
d
```
```{r}
#A word cloud can be created directly from a tm corpus object using the syntax:
wordcloud(d$word, d$freq, min.freq = 90, random.order = FALSE)
```
```{r}
# The above is how to make a wordcloud
# Remember!!! When we use wordcloud(), examine if the target is a dataframe!!
```

```{r}
# Let's take a subset of the train_raw_corpus data by SMS type.
train_raw_spam<-subset(train_raw_sms, type == 'spam')
train_raw_ham<-subset(train_raw_sms, type == 'ham')
train_raw_spam
# This is a dataframe, so we can use wordcloud()
```
```{r}
# Let's see the 40 most frequent words for spam and ham in the raw data
wordcloud(train_raw_spam$text, max.words = 50, scale = c(4,0.5), random.order = FALSE)
wordcloud(train_raw_ham$text, max.words = 50, scale = c(3,0.5), random.order = FALSE)
# max.words parameter to look at the 40 most common words in each of the two sets. The scale parameter allows us to adjust the maximum and minimum font size for words in the cloud.
```

Data Preparation -- creating indicator features for frequent words
```{r}
# Currently, the sparse matrix includes over 7,000 features a feature for every word that appears in at least one SMS message. We will eliminate any words that appear in less than  ve SMS messages, or less than about 0.1 percent of records in the training data.
require(tidyverse)
require(dplyr)
```

```{r}
v_train<-sort(colSums(train_clean_sms), decreasing = FALSE)
fre_indi<-names(v_train[v_train>5])
word_train<-names(v_train)
d_train<-data.frame(word = word_train, frequ = v_train)
d_train_frequent<-filter(d_train, word%in%fre_indi)
d_train_frequent
```
```{r}
v_test<-sort(colSums(test_clean_sms), decreasing = FALSE)
word_test <- names(v_test)
d_test<-data.frame(word = word_test, frequ = v_test)
d_test_frequent<-filter(d_test, word%in%fre_indi)
d_test_frequent
```
```{r}
sms_tokenization<-DocumentTermMatrix(corpus_clean)
sms_dict<-findFreqTerms(sms_tokenization,5)
train_sms_tokenization<-as.matrix(sms_tokenization[train,])
test_sms_tokenization<-as.matrix(sms_tokenization[-train,])
```
```{r}
train_sms_tokenization_frequent<-train_sms_tokenization[,sms_dict]
test_sms_tokenization_frequent<-test_sms_tokenization[,sms_dict]
ncol(test_sms_tokenization_frequent)
```
```{r}
#The naive Bayes classi er is typically trained on data with categorical features. This poses a problem since the cells in the sparse matrix indicate a count of the times a word appears in a message. We should change this to a factor variable that simply indicates yes or no depending on whether the word appears at all.
convert_counts<-function(sparse_matrix_cell){
  sparse_matrix_cell<-ifelse(sparse_matrix_cell>0,1,0)
  sparse_matrix_cell<-factor(sparse_matrix_cell, levels=c(0,1),labels = c("N","Y"))
  return(sparse_matrix_cell)
}
```
```{r}
# Don't rerun this chunk directly, or the value in the cell will be overlapped and the result will be wrong. If have to, then rerun from the chunk before convert_counts chunk.
train_sms_tokenization_frequent<-apply(train_sms_tokenization_frequent, MARGIN = 2, convert_counts)
test_sms_tokenization_frequent<-apply(test_sms_tokenization_frequent, MARGIN = 2, convert_counts)
train_sms_tokenization_frequent[,1:15]
```
```{r}
# Now that we have transformed the raw SMS messages into a format that can be represented by a statistical model, it is time to apply the naive Bayes algorithm. The algorithm will use the presence or absence of words to estimate the probability that a given SMS message is spam.
require(e1071)
```
```{r}
# Unlike the kNN algorithm we used for classi cation in the previous chapter, training a naive Bayes learner and using it for classi cation occur in separate stages.
m<-naiveBayes(train_sms_tokenization_frequent,y=train_raw_sms$type,laplace = 0)
p<-predict(m,test_sms_tokenization_frequent)
```


Evaluating Value Performance
```{r}
require(gmodels)
```
```{r}
CrossTable(p,test_raw_sms$type, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted','actual'))
```





















