---
title: "Divide and Conquer - Classification Using Decision Trees and Rules"
output: html_document
---

Divide and Conquer:
Beginning at the root node, which represents the entire dataset, the algorithm chooses a feature that is the most predictive of the target class. The examples are then partitioned into groups of distinct values of this feature; this decision forms the rest set of tree branches. The algorithm continues to divide-and-conquer the nodes, choosing the best candidate feature each time until a stopping criterion is reached. This might occur at a node if:
• All (or nearly all) of the examples at the node have the same class
• There are no remaining features to distinguish among examples
• The tree has grown to a prede ned size limit

Choosing the Best Split
we looked for feature values that split the data in such a way that partitions contained examples primarily of a single class. If the segments of data contain only a single class, they are considered pure. There are many different measurements of purity for identifying splitting criteria.

C5.0 uses entropy for measuring purity. The entropy of a sample of data indicates how mixed the class values are; the minimum value of 0 indicates that the sample is completely homogenous, while 1 indicates the maximum amount of disorder. The de nition of entropy is speci ed by:
     
              Entropy(S)=sum(-p_i*log_2(p_i),i=1~c)

for a given segment of data (S), the term c refers to the number of different class levels, and pi refers to the proportion of values falling into class level i.
```{r}
curve(-x*log2(x)-(1-x)*log2(1-x),xlab='p_1',ylab='Entropy',col='green')
```
Given this measure of purity, the algorithm must still decide which feature to split upon. For this, the algorithm uses entropy to calculate the change in homogeneity resulting from a split on each possible feature. The calculation is known as information gain. 
                InfoGain(F)= Entropy(S1)−Entropy(S2)


The one complication is that after a split, the data is divided into more than one partition. Therefore, the function to calculate Entropy(S2  needs to consider the total entropy across all of the partitions. It does this by weighing each partition's entropy by the proportion of records falling into that partition. This can be stated in a formula as:
    Total Entropy(S)=sum(w_i*Entropy(p_i),i=1~n)   n is number of class


decision trees use information gain for splitting on numeric features as well. A common practice is testing various splits that divide the values into groups greater than or less than a threshold. This reduces the numeric feature into a two-level categorical feature and information gain can be calculated easily. The numeric threshold yielding the largest information gain is chosen for the split.


Pruning the Tree
1)pre-pruning:stop the tree from growing once it reaches a certain number of decisions or if the decision nodes contain only a small number of examples. 
2)post-pruning: growing a tree that is too large, then using pruning criteria based on the error rates at the nodes to reduce the size of
the tree to a more appropriate level.


Example---identifying risky bank loans using c5.0 decision trees

Step 1--Collecting Data

```{r}
download.file('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data',destfile = '/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/Risky Bank Loans/German.csv')
download.file('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric',destfile = '/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/Risky Bank Loans/German_numeric.csv')
```
```{r}
credit<-read.delim('/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/Risky Bank Loans/German.csv',sep = "",header = FALSE,col.names = c('Status of existing checking account','Duration in month','Credit history','Purpose','Credit amount','Savings account/bonds','Present employment since','Installment rate in percentage of disposable income','Personal status and sex','Other debtors / guarantors','Present residence since','Property','Age in years','Other installment plans','Housing','Number of existing credits at this bank','job','Number of people being liable to provide maintenance for','telephone','foreign worker','default' ))

```
```{r}
credit
```
```{r}
# The checking accounts and saving accounts are probably related to default.
credit$Status.of.existing.checking.account<-factor(credit$Status.of.existing.checking.account, levels=c('A11','A12','A13','A14'), labels = c('<0 DM', '1<=  <200DM','>=200DM','unknown'))
credit$Savings.account.bonds<-factor(credit$Savings.account.bonds,levels=c('A61','A62','A63','A64','A65'),labels = c('<100DM','100<= <500DM','500<= <1000DM','>=1000DM','unkoown'))
table(credit$Status.of.existing.checking.account)
table(credit$Savings.account.bonds)
```
```{r}
# The loan amounts ranged from 250 DM to 18,420 DM across terms of 4 to 72 months, with a median duration of 18 months and amount of 2,320 DM.
summary(credit$Duration.in.month)
summary(credit$Credit.amount)
```
```{r}
# default varaible indicates whether the loan applicant has defaulted.
credit$default<-factor(credit$default,levels=c(1,2),labels=c('no','yes'))
table(credit$default)
```

Data Preparation--creating random training and test datasets
```{r}
set.seed(1019)
train<-sample(1000,900,replace = FALSE)
credit_train<-credit[train,]
credit_test<-credit[-train,]
```

another way to randomly choose train:
randomly order the 'credit' dataset (based on initial dataset is not randomly ordered)
```{r}
set.seed(12345)
credit_rand<-credit[order(runif(1000)),]
credit_rand
```

```{r}
# make sure that the proportion of 'N'&'Y' are close in both datasets
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```


Training a model on the data

```{r}
require(C50)
```
```{r}
# building the classifier
credit_model<-C5.0(credit_train$default~., data = credit_train)
credit_model
```
```{r}
summary(credit_model)
```

```{r}
# Decision trees are known for having a tendency to over t the model to the training data. For this reason, the error rate reported on training data may be overly optimistic, and it is especially important to evaluate decision trees on a test dataset.
```
```{r}
# apply the decision tree
credit_pred<-predict(credit_model,credit_test)
require(gmodels)
CrossTable(credit_test$default,credit_pred,prop.chisq = FALSE,prop.r = FALSE,prop.c = FALSE, dnn=c('actual default','predict default'))
```

Improving Model Performance

1) boosting the accuracy of decision trees
```{r}
# adaptive boosting. This is a process in which many decision trees are built, and the trees vote on the best class for each example.
credit_boost10<-C5.0(credit_train[,-21],credit_train$default,trials = 10)
#summary(credit_boost10)
credit_boost10_pred<-predict(credit_boost10,credit_test)
CrossTable(credit_test$default,credit_boost10_pred,prop.chisq = FALSE,prop.r = FALSE,prop.c = FALSE, dnn=c('actual default','predict default'))
```
2) Making some mistakes more costly than others
```{r}
# The penalties are designated in a cost matrix, which speci es how many times more costly each error is, relative to any other. Suppose we believe that a loan default costs the bank three times as much as a missed opportunity. Our cost matrix then could be de ned as:
error_cost<-matrix(c(0,1,3,0),nrow=2)
error_cost
```
```{r}
# That is, if misclassify a no as yes cost 1, misclassify a yes as no cost 4
# Then apply to C5.0
credit_cost<-C5.0(credit_train[,-21],credit_train$default,trials = 10,costs = error_cost)
credit_cost_pred<-predict(credit_cost,credit_test)
CrossTable(credit_test$default,credit_cost_pred,prop.chisq = FALSE,prop.r = FALSE,prop.c = FALSE, dnn=c('actual default','predict default'))
```



Example--identifying poisonous mushrooms with rule learners

1) Cellecting data
```{r}
download.file('http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data', destfile = '/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/Poisonous Mushrooms/agaricus.csv')
download.file('http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names', destfile = '/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/Poisonous Mushrooms/agaricus_names.txt')
```

2) Exploring and Preparing Data
```{r}
mushrooms<-read.csv('/Users/yisongdong/Desktop/MSF&Python/MSF/Big Data Analysis/machine_learning/Poisonous Mushrooms/agaricus.csv',header = FALSE, stringsAsFactors = TRUE,col.names = c('type','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat'))
mushrooms
```
```{r}
str(mushrooms)
```
```{r}
# veil type only has one value, it is odd.
mushrooms$veil.type<-NULL
mushrooms$type<-factor(mushrooms$type, levels = c('e','p'), labels = c('edible','poisonous'))
mushrooms
table(mushrooms$type)
```

3) Training a model on the data
```{r}
set.seed(1022)
train<-sample(8124,7124)
train_data<-mushrooms[train,]
test_data<-mushrooms[-train,]
train_data
```


```{r}
require(rJava)
require(RWeka)
```
```{r}
mushroom_classifier<-OneR(data=train_data, type~.)
mushroom_predictor<-predict(m, test_data)
```
```{r}
mushroom_classifier
```
```{r}
require(gmodels)
```

4) Evaluating the model performance

```{r}
summary(mushroom_classifier)
```
```{r}
CrossTable(mushroom_predictor, test_data$type, prop.t = FALSE, prop.chisq = FALSE, chisq = FALSE)
```

5) Improving model performance
```{r}
require(RWeka)
```
```{r}
jrip_mushroom_classifier<-JRip(data = train_data, type~.)
jrip_mushroom_prediction<-predict(jrip_mushroom_classifier, test_data)
```

```{r}
jrip_mushroom_classifier
# The first three rules could be expressed as:
#• If the odor is foul, then the mushroom type is poisonous
#• If the gill size is narrow and the gill color is buff, then the mushroom type is poisonous
#• If the gill size is narrow and the odor is pungent, then the mushroom type is poisonous

# Finally, the ninth rule implies that any mushroom sample that was not covered by the preceding eight rules is edible. Following the example of our programming logic, this can be read as:
#• Else, the mushroom is edible
```







